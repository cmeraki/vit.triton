{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7635134",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80966e24-909a-4e7c-914c-a496d4caa4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import ViTModel, ViTConfig\n",
    "\n",
    "from vit.vit import VIT\n",
    "from vit.load_weights import transfer_pretrained_weights\n",
    "\n",
    "from vit.utils import timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6daa88-e14c-437e-b0ff-7951a7d7affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def diff(a, b):\n",
    "    return torch.abs(a - b).mean()\n",
    "\n",
    "device = 'cuda:0'\n",
    "dtype = torch.float32\n",
    "model_id = 'google/vit-base-patch16-224'\n",
    "vit_config = ViTConfig(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f682a17-584b-4ef6-8cb8-2e94c3565777",
   "metadata": {},
   "source": [
    "**Loading weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fdd2b0-86a5-4862-9344-6eaa6e6e2559",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "height, width, channels = vit_config.image_size, vit_config.image_size, vit_config.num_channels\n",
    "patch_size = vit_config.patch_size\n",
    "hidden_dim = 768\n",
    "num_heads=vit_config.num_attention_heads\n",
    "num_layers=vit_config.num_hidden_layers\n",
    "\n",
    "model = VIT(\n",
    "    height=height,\n",
    "    width=width,\n",
    "    channels=channels,\n",
    "    patch_size=patch_size,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers\n",
    ")\n",
    "model.to(device, dtype)\n",
    "\n",
    "pretrained_model = ViTModel.from_pretrained(model_id, add_pooling_layer=False)\n",
    "pretrained_model.to(device, dtype)\n",
    "pretrained_model.eval()\n",
    "\n",
    "model = transfer_pretrained_weights(\n",
    "    pretrained_model=pretrained_model,\n",
    "    custom_model=model\n",
    ")\n",
    "\n",
    "# Number of params in each model\n",
    "sum(p.numel() for p in pretrained_model.parameters()), sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a277197-9233-442e-80f9-c61c3c0721d6",
   "metadata": {},
   "source": [
    "**Verifying layer by layer outputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291b9fc0-0fab-408f-839f-70605b4aae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_pretrained = {}\n",
    "store_custom = {}\n",
    "\n",
    "def hook(module, input, output, name, store):\n",
    "    store[name] = output\n",
    "\n",
    "for name, layer in pretrained_model.named_modules():\n",
    "    layer.register_forward_hook(lambda layer, input, output, name=name: hook(layer, input, output, name, store_pretrained))\n",
    "\n",
    "for name, layer in model.named_modules():\n",
    "    layer.register_forward_hook(lambda layer, input, output, name=name: hook(layer, input, output, name, store_custom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116b5d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn((4, 3, 224, 224)).to(device, dtype)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pretrained_out = pretrained_model(inputs)[0]\n",
    "    custom_out = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1324f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "o1 = timed(pretrained_model, inputs)\n",
    "o2 = timed(model, inputs)\n",
    "\n",
    "print(o2[1]/o1[1], o2[1], o1[1])\n",
    "print(diff(o1[0][0], o2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48034c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d32562-3280-4054-b530-ec527e1cff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_mapping = {\n",
    "    'embeddings.patch_embeddings.projection': 'embeddings.projection',\n",
    "    'embeddings.patch_embeddings': None,\n",
    "    'embeddings.dropout': None,\n",
    "    'embeddings': 'embeddings',\n",
    "    'layernorm': 'layernorm',\n",
    "}\n",
    "\n",
    "for i in range(0, num_heads):\n",
    "    weight_mapping.update({\n",
    "        f'encoder.layer.{i}.layernorm_before': f'encoder.layer.{i}.layernorm_before',\n",
    "        f'encoder.layer.{i}.attention.attention.query': f'encoder.layer.{i}.attention.attention.qkv',\n",
    "        f'encoder.layer.{i}.attention.attention.key': f'encoder.layer.{i}.attention.attention.qkv',\n",
    "        f'encoder.layer.{i}.attention.attention.value': f'encoder.layer.{i}.attention.attention.qkv',\n",
    "        f'encoder.layer.{i}.attention.attention.dropout': None,\n",
    "        f'encoder.layer.{i}.attention.attention': f'encoder.layer.{i}.attention.attention',\n",
    "        f'encoder.layer.{i}.attention.output.dense': None,\n",
    "        f'encoder.layer.{i}.attention.output.dropout': None,\n",
    "        f'encoder.layer.{i}.attention.output': f'encoder.layer.{i}.attention.output',\n",
    "        f'encoder.layer.{i}.attention': f'encoder.layer.{i}.attention',\n",
    "        f'encoder.layer.{i}.layernorm_after': f'encoder.layer.{i}.layernorm_after',\n",
    "        f'encoder.layer.{i}.intermediate.dense': f'encoder.layer.{i}.intermediate',\n",
    "        f'encoder.layer.{i}.intermediate.intermediate_act_fn': None,\n",
    "        f'encoder.layer.{i}.intermediate': None,\n",
    "        f'encoder.layer.{i}.output.dense': f'encoder.layer.{i}.output',\n",
    "        f'encoder.layer.{i}.output.dropout': None,\n",
    "        f'encoder.layer.{i}.output': None,\n",
    "        f'encoder.layer.{i}': f'encoder.layer.{i}',\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6953579-4f98-4218-a80f-410fb1ea2cef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k, v in weight_mapping.items():\n",
    "\n",
    "    if k and v:\n",
    "        if type(v) == list:\n",
    "            val2 = torch.cat([store_custom[e] for e in v], dim=-1)\n",
    "        else:\n",
    "            val2 = store_custom[v]\n",
    "\n",
    "        val1 = store_pretrained[k]\n",
    "\n",
    "        if type(val1) == tuple:\n",
    "            val1 = val1[0]\n",
    "\n",
    "        if val1.shape != val2.shape:\n",
    "             print(f'{k}: {val1.shape}\\t\\t\\t\\t{v}: {val2.shape}')\n",
    "             continue\n",
    "\n",
    "        print(f'{k}: {val1.shape}\\t\\t\\t\\t{v}: {val2.shape}\\t\\t\\t\\t{torch.abs(val1-val2).max()}')\n",
    "\n",
    "        match = torch.allclose(val1, val2, rtol=0, atol=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469fffda",
   "metadata": {},
   "source": [
    "Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9389157",
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 = store_pretrained['encoder.layer.0.layernorm_before']\n",
    "i2 = store_custom['encoder.layer.0.layernorm_before']\n",
    "\n",
    "w1 = pretrained_model.encoder.layer[0].attention.attention.key.weight\n",
    "b1 = pretrained_model.encoder.layer[0].attention.attention.key.bias\n",
    "\n",
    "w2 = model.encoder.layer[0].attention.attention.qkv.weight\n",
    "b2 = model.encoder.layer[0].attention.attention.qkv.bias\n",
    "\n",
    "z1 = torch.matmul(i1, w1.T)\n",
    "z2 = torch.matmul(i2, w2)\n",
    "z2 = z2[:, :, 768:1536]\n",
    "\n",
    "v1 = store_pretrained['encoder.layer.0.attention.attention.key']\n",
    "v2 = store_custom['encoder.layer.0.attention.attention.qkv']\n",
    "v2 = v2[:, :, 768:1536]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f6aa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff(i1, i2), diff(w1.T, w2[:, 768:1536]), diff(b1, b2[768:1536])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3870fa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff(z1, z2), diff(v1, v2), diff(z1, v1), diff(z2, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1ed6e0-2425-4799-8fd6-ddd1519d3715",
   "metadata": {},
   "source": [
    "**Assigning identity weights for debugging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e66497-9f80-473c-a501-74ef2ceeaef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_state_dict = pretrained_model.state_dict()\n",
    "custom_state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d1bb05-e780-440f-9e26-7200958967b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_state_dict_new = {}\n",
    "custom_state_dict_new = {}\n",
    "\n",
    "for k, v in pretrained_state_dict.items():\n",
    "    pretrained_state_dict_new[k] = torch.ones_like(v).to(device, dtype)\n",
    "\n",
    "for k, v in custom_state_dict.items():\n",
    "    custom_state_dict_new[k] = torch.ones_like(v).to(device, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad789b6-2899-40e8-a475-b29e1b69cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(custom_state_dict_new)\n",
    "pretrained_model.load_state_dict(pretrained_state_dict_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
