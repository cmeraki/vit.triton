{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80966e24-909a-4e7c-914c-a496d4caa4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import ViTModel, ViTConfig, AutoImageProcessor\n",
    "\n",
    "from vit.vit import VIT\n",
    "from vit.utils import transfer_pretrained_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b88de65-adc0-407b-844a-b6d94620fe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6daa88-e14c-437e-b0ff-7951a7d7affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(a, b):\n",
    "    return torch.abs(a - b).max()\n",
    "\n",
    "device = 'cuda:0'\n",
    "dtype = torch.float32\n",
    "model_id = 'google/vit-base-patch16-224'\n",
    "vit_config = ViTConfig(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f682a17-584b-4ef6-8cb8-2e94c3565777",
   "metadata": {},
   "source": [
    "**Loading weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fdd2b0-86a5-4862-9344-6eaa6e6e2559",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "height, width, channels = vit_config.image_size, vit_config.image_size, vit_config.num_channels\n",
    "patch_size = vit_config.patch_size\n",
    "hidden_dim = 768\n",
    "num_heads=vit_config.num_attention_heads\n",
    "num_layers=vit_config.num_hidden_layers\n",
    "\n",
    "model = VIT(\n",
    "    height=height,\n",
    "    width=width,\n",
    "    channels=channels,\n",
    "    patch_size=patch_size,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers\n",
    ")\n",
    "model.to(device, dtype)\n",
    "\n",
    "pretrained_model = ViTModel.from_pretrained(model_id, add_pooling_layer=False)\n",
    "pretrained_model.to(device, dtype)\n",
    "pretrained_model.eval()\n",
    "\n",
    "model = transfer_pretrained_weights(\n",
    "    pretrained_model=pretrained_model,\n",
    "    custom_model=model\n",
    ")\n",
    "\n",
    "sum(p.numel() for p in pretrained_model.parameters()), sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ce690b-e93e-4703-99f1-c7c609712ab0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfeab17-491b-41eb-9b0d-566e9fb5ccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a277197-9233-442e-80f9-c61c3c0721d6",
   "metadata": {},
   "source": [
    "**Verifying layer by layer outputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8c9756-c82b-495b-8230-2b46265d46a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a06f09-23fd-44f1-a5b9-95c7a82d6f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image = image.resize((height, width))\n",
    "image = torch.Tensor(np.array(image)).to(device=device, dtype=dtype)\n",
    "\n",
    "inputs = image_processor(image, return_tensors=\"pt\")\n",
    "inputs = inputs['pixel_values'].to(device, dtype)\n",
    "\n",
    "\n",
    "print(f'Input image shape: {image.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291b9fc0-0fab-408f-839f-70605b4aae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_pretrained = {}\n",
    "store_custom = {}\n",
    "\n",
    "def hook(module, input, output, name, store):\n",
    "    store[name] = output\n",
    "\n",
    "for name, layer in pretrained_model.named_modules():\n",
    "    layer.register_forward_hook(lambda layer, input, output, name=name: hook(layer, input, output, name, store_pretrained))\n",
    "\n",
    "for name, layer in model.named_modules():\n",
    "    layer.register_forward_hook(lambda layer, input, output, name=name: hook(layer, input, output, name, store_custom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3ce837-91b2-417d-b154-b9b2234159f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pretrained_out = pretrained_model(inputs)\n",
    "    m1_time = time.time() - start_time\n",
    "    \n",
    "    custom_out = model(inputs)\n",
    "    m2_time = time.time() - m1_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb87bee0-6986-4b0c-9054-02319caa4a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_time, m2_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d32562-3280-4054-b530-ec527e1cff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_mapping = {\n",
    "    'embeddings.patch_embeddings.projection': 'embeddings.projection',\n",
    "    'embeddings.patch_embeddings': None,\n",
    "    'embeddings.dropout': None,\n",
    "    'embeddings': 'embeddings',\n",
    "    'layernorm': 'layernorm',\n",
    "}\n",
    "\n",
    "for i in range(0, num_heads):\n",
    "    weight_mapping.update({\n",
    "        f'encoder.layer.{i}.layernorm_before': f'encoder.layer.{i}.layernorm_before',\n",
    "        f'encoder.layer.{i}.attention.attention.query': [f'encoder.layer.{i}.attention.attention.0.query', f'encoder.layer.{i}.attention.attention.1.query', f'encoder.layer.{i}.attention.attention.2.query', f'encoder.layer.{i}.attention.attention.3.query', f'encoder.layer.{i}.attention.attention.4.query', f'encoder.layer.{i}.attention.attention.5.query', f'encoder.layer.{i}.attention.attention.6.query', f'encoder.layer.{i}.attention.attention.7.query', f'encoder.layer.{i}.attention.attention.8.query', f'encoder.layer.{i}.attention.attention.9.query', f'encoder.layer.{i}.attention.attention.10.query', f'encoder.layer.{i}.attention.attention.11.query'],\n",
    "        f'encoder.layer.{i}.attention.attention.key': [f'encoder.layer.{i}.attention.attention.0.key', f'encoder.layer.{i}.attention.attention.1.key', f'encoder.layer.{i}.attention.attention.2.key', f'encoder.layer.{i}.attention.attention.3.key', f'encoder.layer.{i}.attention.attention.4.key', f'encoder.layer.{i}.attention.attention.5.key', f'encoder.layer.{i}.attention.attention.6.key', f'encoder.layer.{i}.attention.attention.7.key', f'encoder.layer.{i}.attention.attention.8.key', f'encoder.layer.{i}.attention.attention.9.key', f'encoder.layer.{i}.attention.attention.10.key', f'encoder.layer.{i}.attention.attention.11.key'],\n",
    "        f'encoder.layer.{i}.attention.attention.value': [f'encoder.layer.{i}.attention.attention.0.value', f'encoder.layer.{i}.attention.attention.1.value', f'encoder.layer.{i}.attention.attention.2.value', f'encoder.layer.{i}.attention.attention.3.value', f'encoder.layer.{i}.attention.attention.4.value', f'encoder.layer.{i}.attention.attention.5.value', f'encoder.layer.{i}.attention.attention.6.value', f'encoder.layer.{i}.attention.attention.7.value', f'encoder.layer.{i}.attention.attention.8.value', f'encoder.layer.{i}.attention.attention.9.value', f'encoder.layer.{i}.attention.attention.10.value', f'encoder.layer.{i}.attention.attention.11.value'],\n",
    "        f'encoder.layer.{i}.attention.attention.dropout': None,\n",
    "        f'encoder.layer.{i}.attention.attention': [f'encoder.layer.{i}.attention.attention.0', f'encoder.layer.{i}.attention.attention.1', f'encoder.layer.{i}.attention.attention.2', f'encoder.layer.{i}.attention.attention.3', f'encoder.layer.{i}.attention.attention.4', f'encoder.layer.{i}.attention.attention.5', f'encoder.layer.{i}.attention.attention.6', f'encoder.layer.{i}.attention.attention.7', f'encoder.layer.{i}.attention.attention.8', f'encoder.layer.{i}.attention.attention.9', f'encoder.layer.{i}.attention.attention.10', f'encoder.layer.{i}.attention.attention.11'],\n",
    "        f'encoder.layer.{i}.attention.output.dense': None,\n",
    "        f'encoder.layer.{i}.attention.output.dropout': None,\n",
    "        f'encoder.layer.{i}.attention.output': f'encoder.layer.{i}.attention.output',\n",
    "        f'encoder.layer.{i}.attention': f'encoder.layer.{i}.attention',\n",
    "        f'encoder.layer.{i}.layernorm_after': f'encoder.layer.{i}.layernorm_after',\n",
    "        f'encoder.layer.{i}.intermediate.dense': f'encoder.layer.{i}.intermediate',\n",
    "        f'encoder.layer.{i}.intermediate.intermediate_act_fn': None,\n",
    "        f'encoder.layer.{i}.intermediate': None,\n",
    "        f'encoder.layer.{i}.output.dense': f'encoder.layer.{i}.output',\n",
    "        f'encoder.layer.{i}.output.dropout': None,\n",
    "        f'encoder.layer.{i}.output': None,\n",
    "        f'encoder.layer.{i}': f'encoder.layer.{i}',\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6953579-4f98-4218-a80f-410fb1ea2cef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k, v in weight_mapping.items():\n",
    "\n",
    "    if k and v:\n",
    "        if type(v) == list:\n",
    "            val2 = torch.cat([store_custom[e] for e in v], dim=-1)\n",
    "        else:\n",
    "            val2 = store_custom[v]\n",
    "\n",
    "        val1 = store_pretrained[k]\n",
    "\n",
    "        if type(val1) == tuple:\n",
    "            val1 = val1[0]\n",
    "\n",
    "        print(f'{k}\\t{val1.shape}, {v}\\t{val2.shape}\\t{torch.abs(val1-val2).max()}')\n",
    "\n",
    "        match = torch.allclose(val1, val2, rtol=0, atol=1)\n",
    "\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4762b3-20b9-4bf9-97c3-1d9ef379beac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa1acb6-9ab5-4a4e-b6fa-03f9e2821f2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94195af2-0ce3-4a57-9199-58bfd698015a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 = store_pretrained['encoder.layer.4.intermediate.dense']\n",
    "q1 = pretrained_model.encoder.layer[4].output.dense\n",
    "i2 = store_custom['encoder.layer.4.intermediate']\n",
    "q2 = model.encoder.layer[4].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c2e098-0c39-43a3-8c34-55662d643654",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff(q1(i1), q2(i2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24039fa-abe4-4883-9496-d5339fc01713",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1i1 = q1(i1)\n",
    "q1i2 = q1(i2)\n",
    "q2i1 = q2(i1)\n",
    "q2i2 = q2(i2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59d2f8b-e1cb-4ffc-8bc4-0e6f630b15d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff(i1, i2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a149e50-e228-4f38-bb9f-c40cbd50d241",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff(q1i1, q1i2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947aafa0-57ab-42dd-b7c6-16a0f7912606",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff(q1i1, q2i1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aac4d16-f5c9-4421-bd7b-06ef68f92942",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff(q1i2, q2i2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1ed6e0-2425-4799-8fd6-ddd1519d3715",
   "metadata": {},
   "source": [
    "**Assigning identity weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e66497-9f80-473c-a501-74ef2ceeaef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_state_dict = pretrained_model.state_dict()\n",
    "custom_state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d1bb05-e780-440f-9e26-7200958967b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_state_dict_new = {}\n",
    "custom_state_dict_new = {}\n",
    "\n",
    "for k, v in pretrained_state_dict.items():\n",
    "    pretrained_state_dict_new[k] = torch.ones_like(v).to(device, dtype)\n",
    "\n",
    "for k, v in custom_state_dict.items():\n",
    "    custom_state_dict_new[k] = torch.ones_like(v).to(device, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad789b6-2899-40e8-a475-b29e1b69cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(custom_state_dict_new)\n",
    "pretrained_model.load_state_dict(pretrained_state_dict_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcd93ec-9933-4ef5-bd81-8bb2437942c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
